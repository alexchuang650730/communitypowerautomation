"""
MCPé©é…å™¨ä¿¡å¿ƒåº¦æ¸¬è©¦æ¡†æ¶

ç”¨æ–¼å¯¦éš›æ¸¬è©¦å’Œè¨ˆç®—æ¯å€‹MCPé©é…å™¨çš„çœŸå¯¦ä¿¡å¿ƒåº¦
"""

import asyncio
import time
import json
import logging
from typing import Dict, List, Any, Tuple
from dataclasses import dataclass
from pathlib import Path

@dataclass
class TestResult:
    """å–®æ¬¡æ¸¬è©¦çµæœ"""
    adapter_name: str
    test_case: str
    success: bool
    response_time: float
    error_message: str = None
    timestamp: str = None

@dataclass
class ConfidenceMetrics:
    """ä¿¡å¿ƒåº¦æŒ‡æ¨™"""
    functionality_score: float  # åŠŸèƒ½å®Œæ•´æ€§ (0-1)
    stability_score: float      # ç©©å®šæ€§ (0-1) 
    performance_score: float    # æ€§èƒ½ (0-1)
    error_handling_score: float # éŒ¯èª¤è™•ç† (0-1)
    documentation_score: float  # æ–‡æª”å®Œæ•´æ€§ (0-1)
    overall_confidence: float   # ç¸½é«”ä¿¡å¿ƒåº¦ (0-100)

class MCPConfidenceCalculator:
    """MCPé©é…å™¨ä¿¡å¿ƒåº¦è¨ˆç®—å™¨"""
    
    def __init__(self):
        self.test_results: List[TestResult] = []
        self.weights = {
            'functionality': 0.30,
            'stability': 0.25, 
            'performance': 0.20,
            'error_handling': 0.15,
            'documentation': 0.10
        }
        
        # æ€§èƒ½åŸºæº–
        self.performance_thresholds = {
            'excellent': 1.0,    # < 1ç§’
            'good': 3.0,         # < 3ç§’
            'acceptable': 10.0,  # < 10ç§’
            'poor': float('inf') # >= 10ç§’
        }
    
    def test_adapter_functionality(self, adapter_name: str) -> float:
        """æ¸¬è©¦é©é…å™¨åŠŸèƒ½å®Œæ•´æ€§"""
        # é€™è£¡æ‡‰è©²å¯¦ç¾å…·é«”çš„åŠŸèƒ½æ¸¬è©¦
        # ä¾‹å¦‚ï¼šæª¢æŸ¥æ‰€æœ‰å¿…éœ€çš„æ–¹æ³•æ˜¯å¦å¯¦ç¾
        test_cases = [
            "basic_initialization",
            "method_availability", 
            "parameter_validation",
            "output_format_check"
        ]
        
        success_count = 0
        for test_case in test_cases:
            try:
                # æ¨¡æ“¬æ¸¬è©¦åŸ·è¡Œ
                result = self._execute_functionality_test(adapter_name, test_case)
                if result:
                    success_count += 1
                    
                self.test_results.append(TestResult(
                    adapter_name=adapter_name,
                    test_case=f"functionality_{test_case}",
                    success=result,
                    response_time=0.1,
                    timestamp=time.strftime("%Y-%m-%d %H:%M:%S")
                ))
            except Exception as e:
                self.test_results.append(TestResult(
                    adapter_name=adapter_name,
                    test_case=f"functionality_{test_case}",
                    success=False,
                    response_time=0.0,
                    error_message=str(e),
                    timestamp=time.strftime("%Y-%m-%d %H:%M:%S")
                ))
        
        return success_count / len(test_cases)
    
    def test_adapter_stability(self, adapter_name: str, iterations: int = 10) -> float:
        """æ¸¬è©¦é©é…å™¨ç©©å®šæ€§"""
        success_count = 0
        total_time = 0
        
        for i in range(iterations):
            start_time = time.time()
            try:
                # æ¨¡æ“¬ç©©å®šæ€§æ¸¬è©¦
                result = self._execute_stability_test(adapter_name, i)
                response_time = time.time() - start_time
                total_time += response_time
                
                if result:
                    success_count += 1
                    
                self.test_results.append(TestResult(
                    adapter_name=adapter_name,
                    test_case=f"stability_test_{i}",
                    success=result,
                    response_time=response_time,
                    timestamp=time.strftime("%Y-%m-%d %H:%M:%S")
                ))
                
            except Exception as e:
                response_time = time.time() - start_time
                self.test_results.append(TestResult(
                    adapter_name=adapter_name,
                    test_case=f"stability_test_{i}",
                    success=False,
                    response_time=response_time,
                    error_message=str(e),
                    timestamp=time.strftime("%Y-%m-%d %H:%M:%S")
                ))
        
        return success_count / iterations
    
    def test_adapter_performance(self, adapter_name: str) -> float:
        """æ¸¬è©¦é©é…å™¨æ€§èƒ½"""
        test_cases = ["small_request", "medium_request", "large_request"]
        total_score = 0
        
        for test_case in test_cases:
            start_time = time.time()
            try:
                result = self._execute_performance_test(adapter_name, test_case)
                response_time = time.time() - start_time
                
                # æ ¹æ“šéŸ¿æ‡‰æ™‚é–“è¨ˆç®—æ€§èƒ½åˆ†æ•¸
                if response_time < self.performance_thresholds['excellent']:
                    score = 1.0
                elif response_time < self.performance_thresholds['good']:
                    score = 0.8
                elif response_time < self.performance_thresholds['acceptable']:
                    score = 0.6
                else:
                    score = 0.3
                
                total_score += score
                
                self.test_results.append(TestResult(
                    adapter_name=adapter_name,
                    test_case=f"performance_{test_case}",
                    success=result,
                    response_time=response_time,
                    timestamp=time.strftime("%Y-%m-%d %H:%M:%S")
                ))
                
            except Exception as e:
                response_time = time.time() - start_time
                total_score += 0.1  # å¤±æ•—çš„æœ€ä½åˆ†
                
                self.test_results.append(TestResult(
                    adapter_name=adapter_name,
                    test_case=f"performance_{test_case}",
                    success=False,
                    response_time=response_time,
                    error_message=str(e),
                    timestamp=time.strftime("%Y-%m-%d %H:%M:%S")
                ))
        
        return total_score / len(test_cases)
    
    def test_error_handling(self, adapter_name: str) -> float:
        """æ¸¬è©¦éŒ¯èª¤è™•ç†èƒ½åŠ›"""
        error_scenarios = [
            "invalid_input",
            "network_timeout", 
            "service_unavailable",
            "malformed_response"
        ]
        
        success_count = 0
        for scenario in error_scenarios:
            try:
                # æ¸¬è©¦éŒ¯èª¤è™•ç†
                handled_gracefully = self._execute_error_test(adapter_name, scenario)
                if handled_gracefully:
                    success_count += 1
                    
                self.test_results.append(TestResult(
                    adapter_name=adapter_name,
                    test_case=f"error_handling_{scenario}",
                    success=handled_gracefully,
                    response_time=0.1,
                    timestamp=time.strftime("%Y-%m-%d %H:%M:%S")
                ))
                
            except Exception as e:
                self.test_results.append(TestResult(
                    adapter_name=adapter_name,
                    test_case=f"error_handling_{scenario}",
                    success=False,
                    response_time=0.0,
                    error_message=str(e),
                    timestamp=time.strftime("%Y-%m-%d %H:%M:%S")
                ))
        
        return success_count / len(error_scenarios)
    
    def evaluate_documentation(self, adapter_name: str) -> float:
        """è©•ä¼°æ–‡æª”å®Œæ•´æ€§"""
        # æª¢æŸ¥æ–‡æª”è¦ç´ 
        doc_elements = [
            "api_documentation",
            "usage_examples",
            "error_codes",
            "configuration_guide"
        ]
        
        score = 0
        for element in doc_elements:
            if self._check_documentation_element(adapter_name, element):
                score += 1
        
        return score / len(doc_elements)
    
    def calculate_confidence(self, adapter_name: str) -> ConfidenceMetrics:
        """è¨ˆç®—é©é…å™¨çš„ç¶œåˆä¿¡å¿ƒåº¦"""
        
        # åŸ·è¡Œå„é …æ¸¬è©¦
        functionality_score = self.test_adapter_functionality(adapter_name)
        stability_score = self.test_adapter_stability(adapter_name)
        performance_score = self.test_adapter_performance(adapter_name)
        error_handling_score = self.test_error_handling(adapter_name)
        documentation_score = self.evaluate_documentation(adapter_name)
        
        # è¨ˆç®—åŠ æ¬Šç¸½åˆ†
        overall_confidence = (
            functionality_score * self.weights['functionality'] +
            stability_score * self.weights['stability'] +
            performance_score * self.weights['performance'] +
            error_handling_score * self.weights['error_handling'] +
            documentation_score * self.weights['documentation']
        ) * 100
        
        return ConfidenceMetrics(
            functionality_score=functionality_score,
            stability_score=stability_score,
            performance_score=performance_score,
            error_handling_score=error_handling_score,
            documentation_score=documentation_score,
            overall_confidence=overall_confidence
        )
    
    def _execute_functionality_test(self, adapter_name: str, test_case: str) -> bool:
        """åŸ·è¡ŒåŠŸèƒ½æ¸¬è©¦ï¼ˆæ¨¡æ“¬ï¼‰"""
        # é€™è£¡æ‡‰è©²å¯¦ç¾å¯¦éš›çš„åŠŸèƒ½æ¸¬è©¦é‚è¼¯
        # ç›®å‰è¿”å›æ¨¡æ“¬çµæœ
        import random
        return random.random() > 0.1  # 90%æˆåŠŸç‡æ¨¡æ“¬
    
    def _execute_stability_test(self, adapter_name: str, iteration: int) -> bool:
        """åŸ·è¡Œç©©å®šæ€§æ¸¬è©¦ï¼ˆæ¨¡æ“¬ï¼‰"""
        import random
        return random.random() > 0.05  # 95%æˆåŠŸç‡æ¨¡æ“¬
    
    def _execute_performance_test(self, adapter_name: str, test_case: str) -> bool:
        """åŸ·è¡Œæ€§èƒ½æ¸¬è©¦ï¼ˆæ¨¡æ“¬ï¼‰"""
        import random
        time.sleep(random.uniform(0.1, 2.0))  # æ¨¡æ“¬éŸ¿æ‡‰æ™‚é–“
        return random.random() > 0.02  # 98%æˆåŠŸç‡æ¨¡æ“¬
    
    def _execute_error_test(self, adapter_name: str, scenario: str) -> bool:
        """åŸ·è¡ŒéŒ¯èª¤è™•ç†æ¸¬è©¦ï¼ˆæ¨¡æ“¬ï¼‰"""
        import random
        return random.random() > 0.2  # 80%æ­£ç¢ºè™•ç†ç‡æ¨¡æ“¬
    
    def _check_documentation_element(self, adapter_name: str, element: str) -> bool:
        """æª¢æŸ¥æ–‡æª”è¦ç´ ï¼ˆæ¨¡æ“¬ï¼‰"""
        import random
        return random.random() > 0.3  # 70%æ–‡æª”å®Œæ•´ç‡æ¨¡æ“¬
    
    def generate_report(self, output_file: str = None) -> Dict[str, Any]:
        """ç”Ÿæˆä¿¡å¿ƒåº¦æ¸¬è©¦å ±å‘Š"""
        report = {
            "test_summary": {
                "total_tests": len(self.test_results),
                "successful_tests": len([r for r in self.test_results if r.success]),
                "failed_tests": len([r for r in self.test_results if not r.success]),
                "average_response_time": sum(r.response_time for r in self.test_results) / len(self.test_results) if self.test_results else 0
            },
            "test_results": [
                {
                    "adapter_name": r.adapter_name,
                    "test_case": r.test_case,
                    "success": r.success,
                    "response_time": r.response_time,
                    "error_message": r.error_message,
                    "timestamp": r.timestamp
                }
                for r in self.test_results
            ]
        }
        
        if output_file:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
        
        return report

def main():
    """ä¸»å‡½æ•¸ - æ¼”ç¤ºä¿¡å¿ƒåº¦è¨ˆç®—"""
    calculator = MCPConfidenceCalculator()
    
    # æ¸¬è©¦ç¤ºä¾‹é©é…å™¨
    test_adapters = [
        "webagent_adapter",
        "claude_adapter", 
        "gemini_adapter",
        "sequential_thinking_adapter"
    ]
    
    print("ğŸ§ª é–‹å§‹MCPé©é…å™¨ä¿¡å¿ƒåº¦æ¸¬è©¦...")
    print("=" * 60)
    
    for adapter in test_adapters:
        print(f"\nğŸ“Š æ¸¬è©¦é©é…å™¨: {adapter}")
        metrics = calculator.calculate_confidence(adapter)
        
        print(f"  åŠŸèƒ½å®Œæ•´æ€§: {metrics.functionality_score:.2%}")
        print(f"  ç©©å®šæ€§: {metrics.stability_score:.2%}")
        print(f"  æ€§èƒ½: {metrics.performance_score:.2%}")
        print(f"  éŒ¯èª¤è™•ç†: {metrics.error_handling_score:.2%}")
        print(f"  æ–‡æª”å®Œæ•´æ€§: {metrics.documentation_score:.2%}")
        print(f"  âœ… ç¸½é«”ä¿¡å¿ƒåº¦: {metrics.overall_confidence:.1f}%")
    
    # ç”Ÿæˆå ±å‘Š
    report = calculator.generate_report("confidence_test_report.json")
    print(f"\nğŸ“‹ æ¸¬è©¦å ±å‘Šå·²ç”Ÿæˆ: confidence_test_report.json")
    print(f"ç¸½æ¸¬è©¦æ•¸: {report['test_summary']['total_tests']}")
    print(f"æˆåŠŸæ¸¬è©¦: {report['test_summary']['successful_tests']}")
    print(f"å¤±æ•—æ¸¬è©¦: {report['test_summary']['failed_tests']}")

if __name__ == "__main__":
    main()

