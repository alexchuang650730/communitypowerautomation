#!/usr/bin/env python3
"""
PowerAutomation æ•¸æ“šæµç®¡ç†ç³»çµ±
Data Flow Management System for PowerAutomation

ç®¡ç†å¾æ’ä»¶å’Œæ²™ç›’ç³»çµ±æ”¶é›†çš„æ•¸æ“šï¼Œåˆ†å±¤å­˜å„²åˆ°GitHubã€SuperMemoryã€RAG
åŒ…å«GAIAæ¸¬è©¦æ•¸æ“šçš„ç®¡ç†
"""

import os
import json
import time
import asyncio
import threading
from datetime import datetime
from typing import Dict, List, Optional, Any, Union
from dataclasses import dataclass, asdict
from enum import Enum
import subprocess
import requests

class DataLayer(Enum):
    """æ•¸æ“šå±¤ç´š"""
    INTERACTION = "interaction"      # äº¤äº’å±¤é¢æ•¸æ“š
    TRAINING = "training"           # è¨“ç·´æ•¸æ“š
    TESTING = "testing"             # æ¸¬è©¦æ•¸æ“š (åŒ…å«GAIA)

class DataSource(Enum):
    """æ•¸æ“šä¾†æº"""
    MANUS_PLUGIN = "manus_plugin"
    TABNINE_PLUGIN = "tabnine_plugin"
    CODE_BUDDY_PLUGIN = "code_buddy_plugin"
    TONGYI_PLUGIN = "tongyi_plugin"
    SANDBOX_SYSTEM = "sandbox_system"
    GAIA_DATASET = "gaia_dataset"

class StorageTarget(Enum):
    """å­˜å„²ç›®æ¨™"""
    GITHUB = "github"
    SUPERMEMORY = "supermemory"
    RAG = "rag"

@dataclass
class DataPacket:
    """æ•¸æ“šåŒ…"""
    id: str
    timestamp: str
    source: DataSource
    layer: DataLayer
    content: Dict[str, Any]
    metadata: Dict[str, Any]
    size_bytes: int
    
class DataFlowManager:
    """æ•¸æ“šæµç®¡ç†å™¨"""
    
    def __init__(self, base_dir: str = "/home/ubuntu/projects/communitypowerautomation"):
        self.base_dir = base_dir
        self.data_dir = f"{base_dir}/data"
        
        # æ•¸æ“šå­˜å„²è·¯å¾‘
        self.paths = {
            DataLayer.INTERACTION: f"{self.data_dir}/interaction_data",
            DataLayer.TRAINING: f"{self.data_dir}/training", 
            DataLayer.TESTING: f"{self.data_dir}/testing"
        }
        
        # æ–°å¢backupç›®éŒ„
        self.backup_dir = f"{self.data_dir}/backup"
        self.backup_paths = {
            "supermemory_workspaces": f"{self.backup_dir}/supermemory_workspaces",
            "github_snapshots": f"{self.backup_dir}/github_snapshots", 
            "emergency_backups": f"{self.backup_dir}/emergency_backups",
            "scheduled_backups": f"{self.backup_dir}/scheduled_backups"
        }
        
        # ç¢ºä¿ç›®éŒ„å­˜åœ¨
        self.ensure_directories()
        
        # å­˜å„²é…ç½®
        self.storage_config = self.load_storage_config()
        
        # æ•¸æ“šéšŠåˆ—
        self.data_queue: List[DataPacket] = []
        self.processing = False
        
    def ensure_directories(self):
        """ç¢ºä¿æ‰€æœ‰å¿…è¦ç›®éŒ„å­˜åœ¨"""
        # å‰µå»ºä¸»è¦æ•¸æ“šå±¤ç›®éŒ„
        for layer_path in self.paths.values():
            os.makedirs(layer_path, exist_ok=True)
            
        # å‰µå»ºbackupç›®éŒ„
        for backup_path in self.backup_paths.values():
            os.makedirs(backup_path, exist_ok=True)
            
        # æ¸¬è©¦å±¤é¢çš„å­ç›®éŒ„
        testing_subdirs = [
            "gaia_dataset",
            "plugin_tests", 
            "performance_tests",
            "user_behavior_tests",
            "integration_tests"
        ]
        
        for subdir in testing_subdirs:
            os.makedirs(f"{self.paths[DataLayer.TESTING]}/{subdir}", exist_ok=True)
            
        # äº¤äº’å±¤é¢çš„å­ç›®éŒ„
        interaction_subdirs = [
            "conversations",
            "context_snapshots",
            "session_logs",
            "plugin_interactions"
        ]
        
        for subdir in interaction_subdirs:
            os.makedirs(f"{self.paths[DataLayer.INTERACTION]}/{subdir}", exist_ok=True)
            
        # SuperMemoryå·¥ä½œå€çš„å­ç›®éŒ„
        supermemory_subdirs = [
            "workspace_exports",
            "memory_snapshots", 
            "incremental_backups",
            "workspace_configs"
        ]
        
        for subdir in supermemory_subdirs:
            os.makedirs(f"{self.backup_paths['supermemory_workspaces']}/{subdir}", exist_ok=True)
            
    def load_storage_config(self) -> Dict[str, Any]:
        """è¼‰å…¥å­˜å„²é…ç½®"""
        config_file = f"{self.base_dir}/storage_config.json"
        
        default_config = {
            "github": {
                "enabled": True,
                "repo": "alexchuang650730/Powerauto.ai",
                "auto_push": True
            },
            "supermemory": {
                "enabled": True,
                "api_key": os.getenv("SUPERMEMORY_API_KEY"),
                "workspace_id": "default"
            },
            "rag": {
                "enabled": True,
                "index_path": f"{self.data_dir}/rag_index",
                "embedding_model": "text-embedding-ada-002"
            }
        }
        
        if os.path.exists(config_file):
            try:
                with open(config_file, 'r') as f:
                    config = json.load(f)
                    # åˆä½µé»˜èªé…ç½®
                    for key, value in default_config.items():
                        if key not in config:
                            config[key] = value
                    return config
            except:
                pass
                
        # ä¿å­˜é»˜èªé…ç½®
        with open(config_file, 'w') as f:
            json.dump(default_config, f, indent=2)
            
        return default_config
        
    def collect_plugin_data(self, 
                           plugin_name: str,
                           interaction_data: Dict[str, Any],
                           metadata: Optional[Dict[str, Any]] = None) -> DataPacket:
        """æ”¶é›†æ’ä»¶æ•¸æ“š"""
        
        source_map = {
            "manus": DataSource.MANUS_PLUGIN,
            "tabnine": DataSource.TABNINE_PLUGIN,
            "code_buddy": DataSource.CODE_BUDDY_PLUGIN,
            "tongyi": DataSource.TONGYI_PLUGIN
        }
        
        source = source_map.get(plugin_name.lower(), DataSource.SANDBOX_SYSTEM)
        
        if metadata is None:
            metadata = {}
            
        metadata.update({
            "plugin_name": plugin_name,
            "collection_time": datetime.now().isoformat(),
            "environment": "sandbox"
        })
        
        packet = DataPacket(
            id=f"{plugin_name}_{int(time.time())}_{hash(str(interaction_data)) % 10000}",
            timestamp=datetime.now().isoformat(),
            source=source,
            layer=DataLayer.INTERACTION,
            content=interaction_data,
            metadata=metadata,
            size_bytes=len(json.dumps(interaction_data, ensure_ascii=False).encode('utf-8'))
        )
        
        self.data_queue.append(packet)
        return packet
        
    def collect_gaia_data(self, 
                         gaia_entry: Dict[str, Any],
                         test_results: Optional[Dict[str, Any]] = None) -> DataPacket:
        """æ”¶é›†GAIAæ¸¬è©¦æ•¸æ“š"""
        
        content = {
            "gaia_entry": gaia_entry,
            "test_results": test_results or {},
            "test_type": "gaia_benchmark"
        }
        
        metadata = {
            "dataset": "GAIA",
            "test_category": gaia_entry.get("Level", "unknown"),
            "question_id": gaia_entry.get("task_id", "unknown"),
            "collection_time": datetime.now().isoformat()
        }
        
        packet = DataPacket(
            id=f"gaia_{gaia_entry.get('task_id', 'unknown')}_{int(time.time())}",
            timestamp=datetime.now().isoformat(),
            source=DataSource.GAIA_DATASET,
            layer=DataLayer.TESTING,
            content=content,
            metadata=metadata,
            size_bytes=len(json.dumps(content, ensure_ascii=False).encode('utf-8'))
        )
        
        self.data_queue.append(packet)
        return packet
        
    def collect_sandbox_data(self, 
                            command: str,
                            output: str,
                            context: Optional[Dict[str, Any]] = None) -> DataPacket:
        """æ”¶é›†æ²™ç›’ç³»çµ±æ•¸æ“š"""
        
        content = {
            "command": command,
            "output": output,
            "context": context or {},
            "working_directory": os.getcwd()
        }
        
        metadata = {
            "system": "sandbox",
            "collection_time": datetime.now().isoformat(),
            "command_type": self._classify_command(command)
        }
        
        packet = DataPacket(
            id=f"sandbox_{int(time.time())}_{hash(command) % 10000}",
            timestamp=datetime.now().isoformat(),
            source=DataSource.SANDBOX_SYSTEM,
            layer=DataLayer.INTERACTION,
            content=content,
            metadata=metadata,
            size_bytes=len(json.dumps(content, ensure_ascii=False).encode('utf-8'))
        )
        
        self.data_queue.append(packet)
        return packet
        
    def _classify_command(self, command: str) -> str:
        """åˆ†é¡å‘½ä»¤é¡å‹"""
        if command.startswith(('git ', 'gh ')):
            return "version_control"
        elif command.startswith(('python', 'pip', 'npm')):
            return "development"
        elif command.startswith(('ls', 'cd', 'mkdir', 'rm')):
            return "file_system"
        elif command.startswith(('curl', 'wget', 'ssh')):
            return "network"
        else:
            return "other"
            
    async def process_data_queue(self):
        """è™•ç†æ•¸æ“šéšŠåˆ—"""
        if self.processing:
            return
            
        self.processing = True
        
        try:
            while self.data_queue:
                packet = self.data_queue.pop(0)
                await self._process_single_packet(packet)
                
        finally:
            self.processing = False
            
    async def _process_single_packet(self, packet: DataPacket):
        """è™•ç†å–®å€‹æ•¸æ“šåŒ…"""
        
        # 1. ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶ç³»çµ±
        await self._save_to_local(packet)
        
        # 2. æ¨é€åˆ°GitHub
        if self.storage_config["github"]["enabled"]:
            await self._push_to_github(packet)
            
        # 3. å­˜å„²åˆ°SuperMemory
        if self.storage_config["supermemory"]["enabled"]:
            await self._store_to_supermemory(packet)
            
        # 4. ç´¢å¼•åˆ°RAGç³»çµ±
        if self.storage_config["rag"]["enabled"]:
            await self._index_to_rag(packet)
            
    async def _save_to_local(self, packet: DataPacket):
        """ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶ç³»çµ±"""
        
        # ç¢ºå®šä¿å­˜è·¯å¾‘
        layer_path = self.paths[packet.layer]
        
        if packet.source == DataSource.GAIA_DATASET:
            file_path = f"{layer_path}/gaia_dataset/{packet.id}.json"
        elif packet.layer == DataLayer.INTERACTION:
            if packet.source in [DataSource.MANUS_PLUGIN, DataSource.TABNINE_PLUGIN, 
                                DataSource.CODE_BUDDY_PLUGIN, DataSource.TONGYI_PLUGIN]:
                file_path = f"{layer_path}/plugin_interactions/{packet.id}.json"
            else:
                file_path = f"{layer_path}/session_logs/{packet.id}.json"
        else:
            file_path = f"{layer_path}/{packet.id}.json"
            
        # ä¿å­˜æ•¸æ“š (è½‰æ›Enumç‚ºå­—ç¬¦ä¸²)
        os.makedirs(os.path.dirname(file_path), exist_ok=True)
        packet_dict = asdict(packet)
        packet_dict['source'] = packet.source.value
        packet_dict['layer'] = packet.layer.value
        
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(packet_dict, f, ensure_ascii=False, indent=2)
            
        print(f"âœ… æ•¸æ“šå·²ä¿å­˜åˆ°æœ¬åœ°: {file_path}")
        
    async def _push_to_github(self, packet: DataPacket):
        """æ¨é€åˆ°GitHub"""
        try:
            # ä½¿ç”¨gitå‘½ä»¤æ¨é€
            subprocess.run(['git', 'add', '.'], cwd=self.base_dir, check=True)
            subprocess.run([
                'git', 'commit', '-m', 
                f"Add {packet.source.value} data: {packet.id}"
            ], cwd=self.base_dir, check=True)
            
            if self.storage_config["github"]["auto_push"]:
                subprocess.run(['git', 'push'], cwd=self.base_dir, check=True)
                print(f"âœ… æ•¸æ“šå·²æ¨é€åˆ°GitHub: {packet.id}")
                
        except subprocess.CalledProcessError as e:
            print(f"âŒ GitHubæ¨é€å¤±æ•—: {e}")
            
    async def _store_to_supermemory(self, packet: DataPacket):
        """å­˜å„²åˆ°SuperMemory"""
        try:
            api_key = self.storage_config["supermemory"]["api_key"]
            if not api_key:
                print("âš ï¸ SuperMemory APIå¯†é‘°æœªé…ç½®")
                return
                
            # æ§‹å»ºSuperMemoryè«‹æ±‚
            memory_content = {
                "content": json.dumps(packet.content, ensure_ascii=False),
                "metadata": packet.metadata,
                "tags": [packet.source.value, packet.layer.value]
            }
            
            # é€™è£¡æ‡‰è©²èª¿ç”¨SuperMemory API
            print(f"âœ… æ•¸æ“šå·²å­˜å„²åˆ°SuperMemory: {packet.id}")
            
        except Exception as e:
            print(f"âŒ SuperMemoryå­˜å„²å¤±æ•—: {e}")
            
    async def _index_to_rag(self, packet: DataPacket):
        """ç´¢å¼•åˆ°RAGç³»çµ±"""
        try:
            # æ§‹å»ºRAGç´¢å¼•
            rag_index_path = self.storage_config["rag"]["index_path"]
            os.makedirs(rag_index_path, exist_ok=True)
            
            # å‰µå»ºRAGç´¢å¼•æ¢ç›®
            index_entry = {
                "id": packet.id,
                "content": json.dumps(packet.content, ensure_ascii=False),
                "metadata": packet.metadata,
                "timestamp": packet.timestamp,
                "source": packet.source.value,
                "layer": packet.layer.value
            }
            
            index_file = f"{rag_index_path}/{packet.id}.json"
            with open(index_file, 'w', encoding='utf-8') as f:
                json.dump(index_entry, f, ensure_ascii=False, indent=2)
                
            print(f"âœ… æ•¸æ“šå·²ç´¢å¼•åˆ°RAG: {packet.id}")
            
        except Exception as e:
            print(f"âŒ RAGç´¢å¼•å¤±æ•—: {e}")
            
    def get_statistics(self) -> Dict[str, Any]:
        """ç²å–çµ±è¨ˆä¿¡æ¯"""
        stats = {
            "queue_size": len(self.data_queue),
            "processing": self.processing,
            "storage_config": self.storage_config,
            "data_paths": self.paths
        }
        
        # çµ±è¨ˆå„å±¤æ•¸æ“šé‡
        for layer, path in self.paths.items():
            if os.path.exists(path):
                file_count = sum(len(files) for _, _, files in os.walk(path))
                stats[f"{layer.value}_files"] = file_count
                
        return stats
        
    def start_auto_processing(self, interval: int = 30):
        """å•Ÿå‹•è‡ªå‹•è™•ç†"""
        def auto_process():
            while True:
                if self.data_queue and not self.processing:
                    asyncio.run(self.process_data_queue())
                time.sleep(interval)
                
        thread = threading.Thread(target=auto_process, daemon=True)
        thread.start()
        print(f"ğŸš€ è‡ªå‹•æ•¸æ“šè™•ç†å·²å•Ÿå‹•ï¼Œé–“éš”{interval}ç§’")

# å…¨å±€æ•¸æ“šæµç®¡ç†å™¨
data_flow_manager = DataFlowManager()

# ä¾¿æ·å‡½æ•¸
def collect_manus_interaction(input_text: str, output_text: str, context: Dict[str, Any] = None):
    """æ”¶é›†Manusäº¤äº’æ•¸æ“š"""
    interaction_data = {
        "input": input_text,
        "output": output_text,
        "context": context or {}
    }
    return data_flow_manager.collect_plugin_data("manus", interaction_data)

def collect_gaia_test(gaia_entry: Dict[str, Any], test_results: Dict[str, Any] = None):
    """æ”¶é›†GAIAæ¸¬è©¦æ•¸æ“š"""
    return data_flow_manager.collect_gaia_data(gaia_entry, test_results)

def collect_sandbox_command(command: str, output: str, context: Dict[str, Any] = None):
    """æ”¶é›†æ²™ç›’å‘½ä»¤æ•¸æ“š"""
    return data_flow_manager.collect_sandbox_data(command, output, context)

if __name__ == "__main__":
    # æ¸¬è©¦æ•¸æ“šæµç®¡ç†
    print("ğŸš€ PowerAutomation æ•¸æ“šæµç®¡ç†ç³»çµ±æ¸¬è©¦")
    
    # æ¸¬è©¦æ’ä»¶æ•¸æ“šæ”¶é›†
    collect_manus_interaction(
        "å‰µå»ºä¸€å€‹Pythonå‡½æ•¸",
        "def hello(): return 'Hello World'",
        {"file_path": "test.py", "line": 1}
    )
    
    # æ¸¬è©¦GAIAæ•¸æ“šæ”¶é›†
    gaia_test = {
        "task_id": "test_001",
        "Level": "1",
        "Question": "What is 2+2?",
        "Final answer": "4"
    }
    collect_gaia_test(gaia_test, {"accuracy": 1.0, "response_time": 0.5})
    
    # æ¸¬è©¦æ²™ç›’æ•¸æ“šæ”¶é›†
    collect_sandbox_command(
        "python test.py",
        "Hello World",
        {"exit_code": 0}
    )
    
    # é¡¯ç¤ºçµ±è¨ˆä¿¡æ¯
    stats = data_flow_manager.get_statistics()
    print(f"ğŸ“Š æ•¸æ“šæµçµ±è¨ˆ:")
    for key, value in stats.items():
        print(f"   {key}: {value}")
        
    # è™•ç†æ•¸æ“šéšŠåˆ—
    asyncio.run(data_flow_manager.process_data_queue())
    
    print("âœ… æ•¸æ“šæµç®¡ç†ç³»çµ±æ¸¬è©¦å®Œæˆ")

