#!/usr/bin/env python3
"""
RAGæ•´åˆæ¨¡å¡Š (RAG Integration Module)
PowerAutomation è¨˜æ†¶ç³»çµ±çš„èªç¾©æª¢ç´¢å±¤

æä¾›å‘é‡åŒ–ã€èªç¾©æª¢ç´¢å’ŒRAGæ•´åˆåŠŸèƒ½
"""

import os
import json
import numpy as np
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass
import logging
from datetime import datetime
import hashlib

# å˜—è©¦å°å…¥å‘é‡åŒ–ç›¸é—œåº«
try:
    from sentence_transformers import SentenceTransformer
    SENTENCE_TRANSFORMERS_AVAILABLE = True
except ImportError:
    SENTENCE_TRANSFORMERS_AVAILABLE = False
    print("âš ï¸ sentence-transformersæœªå®‰è£ï¼Œå°‡ä½¿ç”¨ç°¡åŒ–çš„å‘é‡åŒ–æ–¹æ³•")

try:
    import faiss
    FAISS_AVAILABLE = True
except ImportError:
    FAISS_AVAILABLE = False
    print("âš ï¸ faissæœªå®‰è£ï¼Œå°‡ä½¿ç”¨ç°¡åŒ–çš„ç›¸ä¼¼åº¦æœç´¢")

# é…ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class VectorizedMemory:
    """å‘é‡åŒ–è¨˜æ†¶"""
    memory_id: str
    content: str
    vector: List[float]
    metadata: Dict[str, Any]
    created_at: str

class SimpleVectorizer:
    """ç°¡åŒ–çš„å‘é‡åŒ–å™¨ï¼ˆç•¶å°ˆæ¥­åº«ä¸å¯ç”¨æ™‚ï¼‰"""
    
    def __init__(self):
        self.vocab = {}
        self.vector_size = 128
        
    def encode(self, texts: List[str]) -> np.ndarray:
        """ç°¡å–®çš„æ–‡æœ¬å‘é‡åŒ–"""
        vectors = []
        
        for text in texts:
            # ç°¡å–®çš„å­—ç¬¦ç´šå‘é‡åŒ–
            vector = np.zeros(self.vector_size)
            
            # åŸºæ–¼å­—ç¬¦é »ç‡çš„å‘é‡åŒ–
            for i, char in enumerate(text[:self.vector_size]):
                vector[i] = ord(char) / 1000.0
                
            # æ­¸ä¸€åŒ–
            norm = np.linalg.norm(vector)
            if norm > 0:
                vector = vector / norm
                
            vectors.append(vector)
            
        return np.array(vectors)

class RAGIntegration:
    """RAGæ•´åˆç®¡ç†å™¨"""
    
    def __init__(self, model_name: str = "all-MiniLM-L6-v2"):
        self.model_name = model_name
        self.vector_db_path = "memory-system/rag-integration/vectors.json"
        self.index_path = "memory-system/rag-integration/faiss.index"
        
        # åˆå§‹åŒ–å‘é‡åŒ–æ¨¡å‹
        self._init_vectorizer()
        
        # åˆå§‹åŒ–å‘é‡æ•¸æ“šåº«
        self.vectors_db = self._load_vectors_db()
        self.faiss_index = self._load_faiss_index()
        
    def _init_vectorizer(self):
        """åˆå§‹åŒ–å‘é‡åŒ–å™¨"""
        if SENTENCE_TRANSFORMERS_AVAILABLE:
            try:
                self.vectorizer = SentenceTransformer(self.model_name)
                self.vector_size = self.vectorizer.get_sentence_embedding_dimension()
                logger.info(f"âœ… ä½¿ç”¨SentenceTransformer: {self.model_name}")
            except Exception as e:
                logger.warning(f"SentenceTransformeråˆå§‹åŒ–å¤±æ•—: {e}")
                self.vectorizer = SimpleVectorizer()
                self.vector_size = self.vectorizer.vector_size
        else:
            self.vectorizer = SimpleVectorizer()
            self.vector_size = self.vectorizer.vector_size
            logger.info("âœ… ä½¿ç”¨ç°¡åŒ–å‘é‡åŒ–å™¨")
            
    def _load_vectors_db(self) -> Dict[str, VectorizedMemory]:
        """åŠ è¼‰å‘é‡æ•¸æ“šåº«"""
        os.makedirs(os.path.dirname(self.vector_db_path), exist_ok=True)
        
        if os.path.exists(self.vector_db_path):
            try:
                with open(self.vector_db_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    
                vectors_db = {}
                for memory_id, memory_data in data.items():
                    vectors_db[memory_id] = VectorizedMemory(
                        memory_id=memory_data['memory_id'],
                        content=memory_data['content'],
                        vector=memory_data['vector'],
                        metadata=memory_data['metadata'],
                        created_at=memory_data['created_at']
                    )
                    
                logger.info(f"âœ… åŠ è¼‰å‘é‡æ•¸æ“šåº«: {len(vectors_db)} æ¢è¨˜éŒ„")
                return vectors_db
                
            except Exception as e:
                logger.error(f"åŠ è¼‰å‘é‡æ•¸æ“šåº«å¤±æ•—: {e}")
                
        return {}
        
    def _save_vectors_db(self):
        """ä¿å­˜å‘é‡æ•¸æ“šåº«"""
        try:
            data = {}
            for memory_id, vectorized_memory in self.vectors_db.items():
                data[memory_id] = {
                    'memory_id': vectorized_memory.memory_id,
                    'content': vectorized_memory.content,
                    'vector': vectorized_memory.vector,
                    'metadata': vectorized_memory.metadata,
                    'created_at': vectorized_memory.created_at
                }
                
            with open(self.vector_db_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
                
            logger.info(f"âœ… ä¿å­˜å‘é‡æ•¸æ“šåº«: {len(data)} æ¢è¨˜éŒ„")
            
        except Exception as e:
            logger.error(f"ä¿å­˜å‘é‡æ•¸æ“šåº«å¤±æ•—: {e}")
            
    def _load_faiss_index(self):
        """åŠ è¼‰FAISSç´¢å¼•"""
        if not FAISS_AVAILABLE:
            return None
            
        try:
            if os.path.exists(self.index_path):
                index = faiss.read_index(self.index_path)
                logger.info(f"âœ… åŠ è¼‰FAISSç´¢å¼•: {index.ntotal} æ¢è¨˜éŒ„")
                return index
        except Exception as e:
            logger.error(f"åŠ è¼‰FAISSç´¢å¼•å¤±æ•—: {e}")
            
        # å‰µå»ºæ–°ç´¢å¼•
        try:
            index = faiss.IndexFlatIP(self.vector_size)  # å…§ç©ç›¸ä¼¼åº¦
            logger.info("âœ… å‰µå»ºæ–°FAISSç´¢å¼•")
            return index
        except Exception as e:
            logger.error(f"å‰µå»ºFAISSç´¢å¼•å¤±æ•—: {e}")
            return None
            
    def _save_faiss_index(self):
        """ä¿å­˜FAISSç´¢å¼•"""
        if self.faiss_index and FAISS_AVAILABLE:
            try:
                faiss.write_index(self.faiss_index, self.index_path)
                logger.info("âœ… ä¿å­˜FAISSç´¢å¼•")
            except Exception as e:
                logger.error(f"ä¿å­˜FAISSç´¢å¼•å¤±æ•—: {e}")
                
    def vectorize_memory(self, memory_id: str, content: str, metadata: Dict[str, Any] = None) -> bool:
        """å‘é‡åŒ–è¨˜æ†¶"""
        try:
            # ç”Ÿæˆå‘é‡
            if SENTENCE_TRANSFORMERS_AVAILABLE and hasattr(self.vectorizer, 'encode'):
                vector = self.vectorizer.encode([content])[0].tolist()
            else:
                vector = self.vectorizer.encode([content])[0].tolist()
                
            # å‰µå»ºå‘é‡åŒ–è¨˜æ†¶
            vectorized_memory = VectorizedMemory(
                memory_id=memory_id,
                content=content,
                vector=vector,
                metadata=metadata or {},
                created_at=datetime.now().isoformat()
            )
            
            # å­˜å„²åˆ°å‘é‡æ•¸æ“šåº«
            self.vectors_db[memory_id] = vectorized_memory
            
            # æ·»åŠ åˆ°FAISSç´¢å¼•
            if self.faiss_index:
                vector_array = np.array([vector], dtype=np.float32)
                self.faiss_index.add(vector_array)
                
            logger.info(f"âœ… è¨˜æ†¶å‘é‡åŒ–å®Œæˆ: {memory_id}")
            return True
            
        except Exception as e:
            logger.error(f"è¨˜æ†¶å‘é‡åŒ–å¤±æ•—: {e}")
            return False
            
    def semantic_search(self, query: str, top_k: int = 10, threshold: float = 0.5) -> List[Tuple[str, float]]:
        """èªç¾©æœç´¢"""
        try:
            # å‘é‡åŒ–æŸ¥è©¢
            if SENTENCE_TRANSFORMERS_AVAILABLE and hasattr(self.vectorizer, 'encode'):
                query_vector = self.vectorizer.encode([query])[0]
            else:
                query_vector = self.vectorizer.encode([query])[0]
                
            results = []
            
            if self.faiss_index and self.faiss_index.ntotal > 0:
                # ä½¿ç”¨FAISSæœç´¢
                query_array = np.array([query_vector], dtype=np.float32)
                scores, indices = self.faiss_index.search(query_array, min(top_k, self.faiss_index.ntotal))
                
                memory_ids = list(self.vectors_db.keys())
                for score, idx in zip(scores[0], indices[0]):
                    if idx < len(memory_ids) and score >= threshold:
                        results.append((memory_ids[idx], float(score)))
                        
            else:
                # ä½¿ç”¨ç°¡å–®ç›¸ä¼¼åº¦è¨ˆç®—
                for memory_id, vectorized_memory in self.vectors_db.items():
                    similarity = self._cosine_similarity(query_vector, vectorized_memory.vector)
                    if similarity >= threshold:
                        results.append((memory_id, similarity))
                        
                # æŒ‰ç›¸ä¼¼åº¦æ’åº
                results.sort(key=lambda x: x[1], reverse=True)
                results = results[:top_k]
                
            logger.info(f"âœ… èªç¾©æœç´¢å®Œæˆ: æŸ¥è©¢='{query}', çµæœ={len(results)}æ¢")
            return results
            
        except Exception as e:
            logger.error(f"èªç¾©æœç´¢å¤±æ•—: {e}")
            return []
            
    def get_similar_memories(self, memory_id: str, top_k: int = 5) -> List[Tuple[str, float]]:
        """ç²å–ç›¸ä¼¼è¨˜æ†¶"""
        if memory_id not in self.vectors_db:
            return []
            
        target_memory = self.vectors_db[memory_id]
        return self.semantic_search(target_memory.content, top_k + 1, threshold=0.3)[1:]  # æ’é™¤è‡ªå·±
        
    def update_memory_vector(self, memory_id: str, new_content: str, metadata: Dict[str, Any] = None) -> bool:
        """æ›´æ–°è¨˜æ†¶å‘é‡"""
        # å…ˆåˆªé™¤èˆŠå‘é‡
        if memory_id in self.vectors_db:
            del self.vectors_db[memory_id]
            
        # é‡æ–°å‘é‡åŒ–
        return self.vectorize_memory(memory_id, new_content, metadata)
        
    def delete_memory_vector(self, memory_id: str) -> bool:
        """åˆªé™¤è¨˜æ†¶å‘é‡"""
        try:
            if memory_id in self.vectors_db:
                del self.vectors_db[memory_id]
                logger.info(f"âœ… åˆªé™¤è¨˜æ†¶å‘é‡: {memory_id}")
                return True
            return False
            
        except Exception as e:
            logger.error(f"åˆªé™¤è¨˜æ†¶å‘é‡å¤±æ•—: {e}")
            return False
            
    def rebuild_index(self) -> bool:
        """é‡å»ºFAISSç´¢å¼•"""
        try:
            if not FAISS_AVAILABLE:
                logger.warning("FAISSä¸å¯ç”¨ï¼Œè·³éç´¢å¼•é‡å»º")
                return True
                
            # å‰µå»ºæ–°ç´¢å¼•
            self.faiss_index = faiss.IndexFlatIP(self.vector_size)
            
            # æ·»åŠ æ‰€æœ‰å‘é‡
            if self.vectors_db:
                vectors = []
                for vectorized_memory in self.vectors_db.values():
                    vectors.append(vectorized_memory.vector)
                    
                vectors_array = np.array(vectors, dtype=np.float32)
                self.faiss_index.add(vectors_array)
                
            logger.info(f"âœ… é‡å»ºFAISSç´¢å¼•: {len(self.vectors_db)} æ¢è¨˜éŒ„")
            return True
            
        except Exception as e:
            logger.error(f"é‡å»ºç´¢å¼•å¤±æ•—: {e}")
            return False
            
    def get_statistics(self) -> Dict[str, Any]:
        """ç²å–RAGçµ±è¨ˆä¿¡æ¯"""
        return {
            "total_vectors": len(self.vectors_db),
            "vector_size": self.vector_size,
            "model_name": self.model_name,
            "faiss_available": FAISS_AVAILABLE,
            "sentence_transformers_available": SENTENCE_TRANSFORMERS_AVAILABLE,
            "faiss_index_size": self.faiss_index.ntotal if self.faiss_index else 0
        }
        
    def save_all(self):
        """ä¿å­˜æ‰€æœ‰æ•¸æ“š"""
        self._save_vectors_db()
        self._save_faiss_index()
        
    def _cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """è¨ˆç®—é¤˜å¼¦ç›¸ä¼¼åº¦"""
        try:
            vec1 = np.array(vec1)
            vec2 = np.array(vec2)
            
            dot_product = np.dot(vec1, vec2)
            norm1 = np.linalg.norm(vec1)
            norm2 = np.linalg.norm(vec2)
            
            if norm1 == 0 or norm2 == 0:
                return 0.0
                
            return dot_product / (norm1 * norm2)
            
        except Exception as e:
            logger.error(f"è¨ˆç®—ç›¸ä¼¼åº¦å¤±æ•—: {e}")
            return 0.0

# å…¨å±€å¯¦ä¾‹
rag_integration = RAGIntegration()

if __name__ == "__main__":
    # æ¸¬è©¦RAGæ•´åˆåŠŸèƒ½
    rag = RAGIntegration()
    
    # æ¸¬è©¦å‘é‡åŒ–
    test_memories = [
        ("mem1", "å¯¦ç¾è¨˜æ†¶å­˜å„²æ¨¡å¡Šï¼Œæä¾›é«˜æ•ˆçš„æ•¸æ“šå­˜å„²åŠŸèƒ½"),
        ("mem2", "å‰µå»ºæ™ºèƒ½åˆ†é¡å™¨ï¼Œè‡ªå‹•åˆ¤æ–·è¨˜æ†¶çš„é‡è¦æ€§"),
        ("mem3", "é–‹ç™¼RAGæ•´åˆç³»çµ±ï¼Œæ”¯æŒèªç¾©æª¢ç´¢"),
        ("mem4", "ä¿®å¾©MCPé©é…å™¨çš„è¨»å†Šå•é¡Œ"),
        ("mem5", "å¯¦ç¾ZIPåŠ å¯†çš„tokenç®¡ç†ç³»çµ±")
    ]
    
    print("ğŸ§ª æ¸¬è©¦RAGæ•´åˆåŠŸèƒ½...")
    
    # å‘é‡åŒ–æ¸¬è©¦è¨˜æ†¶
    for memory_id, content in test_memories:
        success = rag.vectorize_memory(memory_id, content, {"test": True})
        if success:
            print(f"âœ… å‘é‡åŒ–æˆåŠŸ: {memory_id}")
        else:
            print(f"âŒ å‘é‡åŒ–å¤±æ•—: {memory_id}")
            
    # æ¸¬è©¦èªç¾©æœç´¢
    query = "è¨˜æ†¶å­˜å„²å’Œæ•¸æ“šç®¡ç†"
    results = rag.semantic_search(query, top_k=3)
    print(f"\nğŸ” èªç¾©æœç´¢çµæœ (æŸ¥è©¢: '{query}'):")
    for memory_id, score in results:
        print(f"  {memory_id}: {score:.3f}")
        
    # æ¸¬è©¦ç›¸ä¼¼è¨˜æ†¶æŸ¥æ‰¾
    similar = rag.get_similar_memories("mem1", top_k=2)
    print(f"\nğŸ”— ç›¸ä¼¼è¨˜æ†¶ (åŸºæ–¼ mem1):")
    for memory_id, score in similar:
        print(f"  {memory_id}: {score:.3f}")
        
    # ç²å–çµ±è¨ˆä¿¡æ¯
    stats = rag.get_statistics()
    print(f"\nğŸ“Š RAGçµ±è¨ˆä¿¡æ¯: {stats}")
    
    # ä¿å­˜æ•¸æ“š
    rag.save_all()
    print("ğŸ’¾ æ•¸æ“šå·²ä¿å­˜")

