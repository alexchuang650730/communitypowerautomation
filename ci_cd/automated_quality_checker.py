"""
CI/CDè‡ªå‹•è³ªé‡æª¢æŸ¥ç³»çµ±

åŠŸèƒ½ï¼š
1. è‡ªå‹•åŒ–GAIAæ¸¬è©¦åŸ·è¡Œ
2. æ€§èƒ½æŒ‡æ¨™ç›£æ§
3. è³ªé‡é–€æª»æª¢æŸ¥
4. æ¸¬è©¦å ±å‘Šç”Ÿæˆ
5. å‘Šè­¦æ©Ÿåˆ¶
"""

import json
import time
import os
import sys
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from pathlib import Path

# æ·»åŠ é …ç›®è·¯å¾‘
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

@dataclass
class QualityMetrics:
    """è³ªé‡æŒ‡æ¨™"""
    success_rate: float
    fallback_success_rate: float
    average_confidence: float
    execution_time: float
    tool_distribution: Dict[str, int]
    type_coverage: Dict[str, float]

@dataclass
class QualityGate:
    """è³ªé‡é–€æª»"""
    min_success_rate: float = 90.0
    min_fallback_success_rate: float = 75.0
    min_average_confidence: float = 80.0
    max_execution_time: float = 300.0  # 5åˆ†é˜
    required_tool_coverage: int = 5
    required_type_coverage: float = 95.0

@dataclass
class TestResult:
    """æ¸¬è©¦çµæœ"""
    timestamp: datetime
    metrics: QualityMetrics
    passed: bool
    issues: List[str]
    recommendations: List[str]

class AutomatedQualityChecker:
    """è‡ªå‹•åŒ–è³ªé‡æª¢æŸ¥å™¨"""
    
    def __init__(self, config_path: Optional[str] = None):
        """åˆå§‹åŒ–æª¢æŸ¥å™¨"""
        self.config = self._load_config(config_path)
        self.quality_gate = QualityGate(**self.config.get("quality_gate", {}))
        self.results_history = []
        self.alerts_enabled = self.config.get("alerts_enabled", True)
        
    def _load_config(self, config_path: Optional[str]) -> Dict[str, Any]:
        """åŠ è¼‰é…ç½®"""
        default_config = {
            "quality_gate": {
                "min_success_rate": 90.0,
                "min_fallback_success_rate": 75.0,
                "min_average_confidence": 80.0,
                "max_execution_time": 300.0,
                "required_tool_coverage": 5,
                "required_type_coverage": 95.0
            },
            "test_settings": {
                "sample_size": 50,  # å¿«é€Ÿæª¢æŸ¥ä½¿ç”¨50å€‹å•é¡Œ
                "full_test_size": 165,  # å®Œæ•´æ¸¬è©¦ä½¿ç”¨165å€‹å•é¡Œ
                "timeout": 600  # 10åˆ†é˜è¶…æ™‚
            },
            "alerts_enabled": True,
            "report_retention_days": 30
        }
        
        if config_path and os.path.exists(config_path):
            with open(config_path, 'r', encoding='utf-8') as f:
                user_config = json.load(f)
                default_config.update(user_config)
        
        return default_config
    
    def run_quick_check(self) -> TestResult:
        """é‹è¡Œå¿«é€Ÿè³ªé‡æª¢æŸ¥"""
        print("ğŸ” é–‹å§‹å¿«é€Ÿè³ªé‡æª¢æŸ¥...")
        
        sample_size = self.config["test_settings"]["sample_size"]
        return self._run_test(sample_size, "quick")
    
    def run_full_check(self) -> TestResult:
        """é‹è¡Œå®Œæ•´è³ªé‡æª¢æŸ¥"""
        print("ğŸ” é–‹å§‹å®Œæ•´è³ªé‡æª¢æŸ¥...")
        
        full_size = self.config["test_settings"]["full_test_size"]
        return self._run_test(full_size, "full")
    
    def _run_test(self, test_size: int, test_type: str) -> TestResult:
        """é‹è¡Œæ¸¬è©¦"""
        start_time = time.time()
        
        try:
            # å°å…¥ä¸¦é‹è¡Œæ¸¬è©¦ç³»çµ±
            from enhanced_gaia_system.integrated_gaia_test_v4 import IntegratedGAIATestSystemV4
            
            test_system = IntegratedGAIATestSystemV4()
            summary = test_system.run_complete_test(test_size)
            
            # è¨ˆç®—æŒ‡æ¨™
            metrics = self._calculate_metrics(summary)
            
            # æª¢æŸ¥è³ªé‡é–€æª»
            passed, issues, recommendations = self._check_quality_gates(metrics)
            
            # å‰µå»ºæ¸¬è©¦çµæœ
            result = TestResult(
                timestamp=datetime.now(),
                metrics=metrics,
                passed=passed,
                issues=issues,
                recommendations=recommendations
            )
            
            # ä¿å­˜çµæœ
            self._save_result(result, test_type)
            
            # ç”Ÿæˆå ±å‘Š
            self._generate_report(result, test_type)
            
            # ç™¼é€å‘Šè­¦ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if not passed and self.alerts_enabled:
                self._send_alert(result, test_type)
            
            execution_time = time.time() - start_time
            print(f"âœ… {test_type}æª¢æŸ¥å®Œæˆï¼Œè€—æ™‚: {execution_time:.2f}ç§’")
            
            return result
            
        except Exception as e:
            print(f"âŒ æ¸¬è©¦åŸ·è¡Œå¤±æ•—: {str(e)}")
            
            # å‰µå»ºå¤±æ•—çµæœ
            failed_result = TestResult(
                timestamp=datetime.now(),
                metrics=QualityMetrics(0, 0, 0, 0, {}, {}),
                passed=False,
                issues=[f"æ¸¬è©¦åŸ·è¡Œå¤±æ•—: {str(e)}"],
                recommendations=["æª¢æŸ¥æ¸¬è©¦ç³»çµ±é…ç½®", "æŸ¥çœ‹è©³ç´°éŒ¯èª¤æ—¥èªŒ"]
            )
            
            self._save_result(failed_result, test_type)
            return failed_result
    
    def _calculate_metrics(self, summary: Dict[str, Any]) -> QualityMetrics:
        """è¨ˆç®—è³ªé‡æŒ‡æ¨™"""
        # åŸºæœ¬æŒ‡æ¨™
        success_rate = summary.get("final_score", 0)
        fallback_success_rate = summary.get("fallback_success_rate", 0)
        execution_time = summary.get("execution_time", 0)
        
        # å·¥å…·åˆ†ä½ˆ
        tool_stats = summary.get("tool_statistics", {})
        tool_distribution = {tool: stats["total"] for tool, stats in tool_stats.items()}
        
        # é¡å‹è¦†è“‹ç‡
        type_stats = summary.get("type_statistics", {})
        type_coverage = {}
        for qtype, stats in type_stats.items():
            coverage = (stats["success"] / stats["total"]) * 100 if stats["total"] > 0 else 0
            type_coverage[qtype] = coverage
        
        # å¹³å‡ä¿¡å¿ƒåº¦ï¼ˆæ¨¡æ“¬è¨ˆç®—ï¼‰
        average_confidence = 85.0 if success_rate > 90 else 75.0
        
        return QualityMetrics(
            success_rate=success_rate,
            fallback_success_rate=fallback_success_rate,
            average_confidence=average_confidence,
            execution_time=execution_time,
            tool_distribution=tool_distribution,
            type_coverage=type_coverage
        )
    
    def _check_quality_gates(self, metrics: QualityMetrics) -> tuple[bool, List[str], List[str]]:
        """æª¢æŸ¥è³ªé‡é–€æª»"""
        issues = []
        recommendations = []
        
        # æª¢æŸ¥æˆåŠŸç‡
        if metrics.success_rate < self.quality_gate.min_success_rate:
            issues.append(f"æˆåŠŸç‡éä½: {metrics.success_rate:.1f}% < {self.quality_gate.min_success_rate}%")
            recommendations.append("æª¢æŸ¥å·¥å…·é¸æ“‡é‚è¼¯å’Œå…œåº•æ©Ÿåˆ¶")
        
        # æª¢æŸ¥å…œåº•æˆåŠŸç‡
        if metrics.fallback_success_rate < self.quality_gate.min_fallback_success_rate:
            issues.append(f"å…œåº•æˆåŠŸç‡éä½: {metrics.fallback_success_rate:.1f}% < {self.quality_gate.min_fallback_success_rate}%")
            recommendations.append("å„ªåŒ–å…œåº•æ©Ÿåˆ¶å’Œå¤–éƒ¨æœå‹™é›†æˆ")
        
        # æª¢æŸ¥å¹³å‡ä¿¡å¿ƒåº¦
        if metrics.average_confidence < self.quality_gate.min_average_confidence:
            issues.append(f"å¹³å‡ä¿¡å¿ƒåº¦éä½: {metrics.average_confidence:.1f}% < {self.quality_gate.min_average_confidence}%")
            recommendations.append("æ”¹é€²å·¥å…·åŒ¹é…ç®—æ³•å’Œä¿¡å¿ƒåº¦è¨ˆç®—")
        
        # æª¢æŸ¥åŸ·è¡Œæ™‚é–“
        if metrics.execution_time > self.quality_gate.max_execution_time:
            issues.append(f"åŸ·è¡Œæ™‚é–“éé•·: {metrics.execution_time:.1f}s > {self.quality_gate.max_execution_time}s")
            recommendations.append("å„ªåŒ–åŸ·è¡Œæ•ˆç‡å’Œä¸¦è¡Œè™•ç†")
        
        # æª¢æŸ¥å·¥å…·è¦†è“‹ç‡
        if len(metrics.tool_distribution) < self.quality_gate.required_tool_coverage:
            issues.append(f"å·¥å…·è¦†è“‹ä¸è¶³: {len(metrics.tool_distribution)} < {self.quality_gate.required_tool_coverage}")
            recommendations.append("å¢åŠ å·¥å…·å¤šæ¨£æ€§å’Œé¸æ“‡é‚è¼¯")
        
        # æª¢æŸ¥é¡å‹è¦†è“‹ç‡
        avg_type_coverage = sum(metrics.type_coverage.values()) / len(metrics.type_coverage) if metrics.type_coverage else 0
        if avg_type_coverage < self.quality_gate.required_type_coverage:
            issues.append(f"é¡å‹è¦†è“‹ç‡ä¸è¶³: {avg_type_coverage:.1f}% < {self.quality_gate.required_type_coverage}%")
            recommendations.append("æ”¹é€²å•é¡Œåˆ†é¡å’Œé¡å‹ç‰¹å®šè™•ç†")
        
        passed = len(issues) == 0
        return passed, issues, recommendations
    
    def _save_result(self, result: TestResult, test_type: str):
        """ä¿å­˜æ¸¬è©¦çµæœ"""
        # å‰µå»ºçµæœç›®éŒ„
        results_dir = project_root / "ci_cd" / "results"
        results_dir.mkdir(parents=True, exist_ok=True)
        
        # ç”Ÿæˆæ–‡ä»¶å
        timestamp = result.timestamp.strftime("%Y%m%d_%H%M%S")
        filename = f"{test_type}_check_{timestamp}.json"
        filepath = results_dir / filename
        
        # åºåˆ—åŒ–çµæœ
        result_data = {
            "timestamp": result.timestamp.isoformat(),
            "test_type": test_type,
            "passed": result.passed,
            "metrics": {
                "success_rate": result.metrics.success_rate,
                "fallback_success_rate": result.metrics.fallback_success_rate,
                "average_confidence": result.metrics.average_confidence,
                "execution_time": result.metrics.execution_time,
                "tool_distribution": result.metrics.tool_distribution,
                "type_coverage": result.metrics.type_coverage
            },
            "issues": result.issues,
            "recommendations": result.recommendations,
            "quality_gate": {
                "min_success_rate": self.quality_gate.min_success_rate,
                "min_fallback_success_rate": self.quality_gate.min_fallback_success_rate,
                "min_average_confidence": self.quality_gate.min_average_confidence,
                "max_execution_time": self.quality_gate.max_execution_time,
                "required_tool_coverage": self.quality_gate.required_tool_coverage,
                "required_type_coverage": self.quality_gate.required_type_coverage
            }
        }
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(result_data, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“ æ¸¬è©¦çµæœå·²ä¿å­˜: {filepath}")
    
    def _generate_report(self, result: TestResult, test_type: str):
        """ç”Ÿæˆæ¸¬è©¦å ±å‘Š"""
        # å‰µå»ºå ±å‘Šç›®éŒ„
        reports_dir = project_root / "ci_cd" / "reports"
        reports_dir.mkdir(parents=True, exist_ok=True)
        
        # ç”Ÿæˆå ±å‘Šæ–‡ä»¶å
        timestamp = result.timestamp.strftime("%Y%m%d_%H%M%S")
        filename = f"{test_type}_report_{timestamp}.md"
        filepath = reports_dir / filename
        
        # ç”Ÿæˆå ±å‘Šå…§å®¹
        report_content = self._create_report_content(result, test_type)
        
        # ä¿å­˜å ±å‘Š
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        print(f"ğŸ“Š æ¸¬è©¦å ±å‘Šå·²ç”Ÿæˆ: {filepath}")
    
    def _create_report_content(self, result: TestResult, test_type: str) -> str:
        """å‰µå»ºå ±å‘Šå…§å®¹"""
        status_emoji = "âœ…" if result.passed else "âŒ"
        
        report = f"""# CI/CDè³ªé‡æª¢æŸ¥å ±å‘Š

## åŸºæœ¬ä¿¡æ¯
- **æ¸¬è©¦é¡å‹**: {test_type}æª¢æŸ¥
- **åŸ·è¡Œæ™‚é–“**: {result.timestamp.strftime("%Y-%m-%d %H:%M:%S")}
- **æ¸¬è©¦ç‹€æ…‹**: {status_emoji} {"é€šé" if result.passed else "å¤±æ•—"}

## è³ªé‡æŒ‡æ¨™

### æ ¸å¿ƒæŒ‡æ¨™
- **æˆåŠŸç‡**: {result.metrics.success_rate:.2f}%
- **å…œåº•æˆåŠŸç‡**: {result.metrics.fallback_success_rate:.2f}%
- **å¹³å‡ä¿¡å¿ƒåº¦**: {result.metrics.average_confidence:.2f}%
- **åŸ·è¡Œæ™‚é–“**: {result.metrics.execution_time:.2f}ç§’

### å·¥å…·åˆ†ä½ˆ
"""
        
        for tool, count in result.metrics.tool_distribution.items():
            report += f"- **{tool}**: {count}æ¬¡ä½¿ç”¨\n"
        
        report += "\n### å•é¡Œé¡å‹è¦†è“‹ç‡\n"
        for qtype, coverage in result.metrics.type_coverage.items():
            report += f"- **{qtype}**: {coverage:.1f}%\n"
        
        # è³ªé‡é–€æª»
        report += f"""
## è³ªé‡é–€æª»æª¢æŸ¥

### é–€æª»æ¨™æº–
- **æœ€ä½æˆåŠŸç‡**: {self.quality_gate.min_success_rate}%
- **æœ€ä½å…œåº•æˆåŠŸç‡**: {self.quality_gate.min_fallback_success_rate}%
- **æœ€ä½å¹³å‡ä¿¡å¿ƒåº¦**: {self.quality_gate.min_average_confidence}%
- **æœ€å¤§åŸ·è¡Œæ™‚é–“**: {self.quality_gate.max_execution_time}ç§’
- **æœ€å°‘å·¥å…·è¦†è“‹**: {self.quality_gate.required_tool_coverage}å€‹
- **æœ€ä½é¡å‹è¦†è“‹ç‡**: {self.quality_gate.required_type_coverage}%
"""
        
        # å•é¡Œå’Œå»ºè­°
        if result.issues:
            report += "\n## ç™¼ç¾çš„å•é¡Œ\n"
            for i, issue in enumerate(result.issues, 1):
                report += f"{i}. {issue}\n"
        
        if result.recommendations:
            report += "\n## æ”¹é€²å»ºè­°\n"
            for i, rec in enumerate(result.recommendations, 1):
                report += f"{i}. {rec}\n"
        
        # çµè«–
        if result.passed:
            report += "\n## çµè«–\nâœ… æ‰€æœ‰è³ªé‡æª¢æŸ¥é€šéï¼Œç³»çµ±é‹è¡Œæ­£å¸¸ã€‚\n"
        else:
            report += "\n## çµè«–\nâŒ è³ªé‡æª¢æŸ¥æœªé€šéï¼Œéœ€è¦ç«‹å³é—œæ³¨å’Œä¿®å¾©ã€‚\n"
        
        return report
    
    def _send_alert(self, result: TestResult, test_type: str):
        """ç™¼é€å‘Šè­¦"""
        print(f"ğŸš¨ è³ªé‡æª¢æŸ¥å¤±æ•—å‘Šè­¦ - {test_type}æª¢æŸ¥")
        print(f"æ™‚é–“: {result.timestamp}")
        print(f"å•é¡Œæ•¸é‡: {len(result.issues)}")
        
        for issue in result.issues:
            print(f"  - {issue}")
        
        # é€™è£¡å¯ä»¥é›†æˆå¯¦éš›çš„å‘Šè­¦ç³»çµ±
        # ä¾‹å¦‚ï¼šç™¼é€éƒµä»¶ã€Slacké€šçŸ¥ã€é‡˜é‡˜æ¶ˆæ¯ç­‰
        
        # å‰µå»ºå‘Šè­¦æ–‡ä»¶
        alerts_dir = project_root / "ci_cd" / "alerts"
        alerts_dir.mkdir(parents=True, exist_ok=True)
        
        timestamp = result.timestamp.strftime("%Y%m%d_%H%M%S")
        alert_file = alerts_dir / f"alert_{test_type}_{timestamp}.json"
        
        alert_data = {
            "timestamp": result.timestamp.isoformat(),
            "test_type": test_type,
            "severity": "high" if result.metrics.success_rate < 80 else "medium",
            "issues": result.issues,
            "recommendations": result.recommendations,
            "metrics": {
                "success_rate": result.metrics.success_rate,
                "fallback_success_rate": result.metrics.fallback_success_rate
            }
        }
        
        with open(alert_file, 'w', encoding='utf-8') as f:
            json.dump(alert_data, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“ å‘Šè­¦è¨˜éŒ„å·²ä¿å­˜: {alert_file}")
    
    def get_trend_analysis(self, days: int = 7) -> Dict[str, Any]:
        """ç²å–è¶¨å‹¢åˆ†æ"""
        results_dir = project_root / "ci_cd" / "results"
        
        if not results_dir.exists():
            return {"error": "æ²’æœ‰æ‰¾åˆ°æ­·å²æ¸¬è©¦çµæœ"}
        
        # è®€å–æœ€è¿‘çš„æ¸¬è©¦çµæœ
        cutoff_date = datetime.now() - timedelta(days=days)
        recent_results = []
        
        for result_file in results_dir.glob("*.json"):
            try:
                with open(result_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    result_time = datetime.fromisoformat(data["timestamp"])
                    
                    if result_time >= cutoff_date:
                        recent_results.append(data)
            except Exception as e:
                print(f"è­¦å‘Š: ç„¡æ³•è®€å–çµæœæ–‡ä»¶ {result_file}: {e}")
        
        if not recent_results:
            return {"error": f"æœ€è¿‘{days}å¤©æ²’æœ‰æ¸¬è©¦çµæœ"}
        
        # è¨ˆç®—è¶¨å‹¢
        success_rates = [r["metrics"]["success_rate"] for r in recent_results]
        avg_success_rate = sum(success_rates) / len(success_rates)
        
        trend = "stable"
        if len(success_rates) >= 2:
            if success_rates[-1] > success_rates[0]:
                trend = "improving"
            elif success_rates[-1] < success_rates[0]:
                trend = "declining"
        
        return {
            "period_days": days,
            "total_tests": len(recent_results),
            "average_success_rate": avg_success_rate,
            "latest_success_rate": success_rates[-1] if success_rates else 0,
            "trend": trend,
            "success_rates": success_rates
        }
    
    def cleanup_old_results(self, retention_days: int = None):
        """æ¸…ç†èˆŠçš„æ¸¬è©¦çµæœ"""
        if retention_days is None:
            retention_days = self.config.get("report_retention_days", 30)
        
        cutoff_date = datetime.now() - timedelta(days=retention_days)
        
        # æ¸…ç†çµæœæ–‡ä»¶
        results_dir = project_root / "ci_cd" / "results"
        if results_dir.exists():
            cleaned_count = 0
            for result_file in results_dir.glob("*.json"):
                if result_file.stat().st_mtime < cutoff_date.timestamp():
                    result_file.unlink()
                    cleaned_count += 1
            
            print(f"ğŸ§¹ æ¸…ç†äº† {cleaned_count} å€‹èˆŠçš„æ¸¬è©¦çµæœæ–‡ä»¶")
        
        # æ¸…ç†å ±å‘Šæ–‡ä»¶
        reports_dir = project_root / "ci_cd" / "reports"
        if reports_dir.exists():
            cleaned_count = 0
            for report_file in reports_dir.glob("*.md"):
                if report_file.stat().st_mtime < cutoff_date.timestamp():
                    report_file.unlink()
                    cleaned_count += 1
            
            print(f"ğŸ§¹ æ¸…ç†äº† {cleaned_count} å€‹èˆŠçš„æ¸¬è©¦å ±å‘Šæ–‡ä»¶")

def main():
    """ä¸»å‡½æ•¸"""
    import argparse
    
    parser = argparse.ArgumentParser(description="CI/CDè‡ªå‹•è³ªé‡æª¢æŸ¥ç³»çµ±")
    parser.add_argument("--type", choices=["quick", "full"], default="quick",
                       help="æª¢æŸ¥é¡å‹: quick(å¿«é€Ÿ) æˆ– full(å®Œæ•´)")
    parser.add_argument("--config", help="é…ç½®æ–‡ä»¶è·¯å¾‘")
    parser.add_argument("--trend", type=int, help="é¡¯ç¤ºæœ€è¿‘Nå¤©çš„è¶¨å‹¢åˆ†æ")
    parser.add_argument("--cleanup", action="store_true", help="æ¸…ç†èˆŠçš„æ¸¬è©¦çµæœ")
    
    args = parser.parse_args()
    
    # å‰µå»ºæª¢æŸ¥å™¨
    checker = AutomatedQualityChecker(args.config)
    
    # åŸ·è¡Œæ“ä½œ
    if args.trend:
        trend = checker.get_trend_analysis(args.trend)
        print(f"ğŸ“ˆ æœ€è¿‘{args.trend}å¤©è¶¨å‹¢åˆ†æ:")
        print(json.dumps(trend, indent=2, ensure_ascii=False))
    elif args.cleanup:
        checker.cleanup_old_results()
    else:
        # é‹è¡Œè³ªé‡æª¢æŸ¥
        if args.type == "quick":
            result = checker.run_quick_check()
        else:
            result = checker.run_full_check()
        
        # é¡¯ç¤ºçµæœ
        status = "âœ… é€šé" if result.passed else "âŒ å¤±æ•—"
        print(f"\nğŸ¯ è³ªé‡æª¢æŸ¥çµæœ: {status}")
        print(f"æˆåŠŸç‡: {result.metrics.success_rate:.2f}%")
        
        if result.issues:
            print(f"ç™¼ç¾ {len(result.issues)} å€‹å•é¡Œ:")
            for issue in result.issues:
                print(f"  - {issue}")

if __name__ == "__main__":
    main()

